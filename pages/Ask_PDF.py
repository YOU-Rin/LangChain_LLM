from dotenv import load_dotenv
import streamlit as st
from PyPDF2 import PdfReader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI
from langchain.callbacks import get_openai_callback
from utils import PLATFORMS, get_llm_models, get_chatllm, get_img_base64


CHAT_PAGE_INTRODUCTION = "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„ KK_chat æ™ºèƒ½åŠ©æ‰‹ï¼Œå½“å‰é¡µé¢ä¸º`å¯¹è¯æ¨¡å¼`ï¼Œå¯ä»¥ç›´æ¥ä¸å¤§æ¨¡å‹å¯¹è¯ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"


def display_chat_history():
    for message in st.session_state["chat_history"]:
        with st.chat_message(message["role"], avatar=get_img_base64("stitch.png") if message["role"] == "assistant" else None):
            st.write(message["content"])

# æ¸…ç©ºå†å²å¯¹è¯ä¿¡æ¯
"""
æ¸…ç©ºå½“å‰ä¼šè¯çŠ¶æ€ä¸­çš„èŠå¤©å†å²
é‡æ–°æ·»åŠ æ¬¢è¿æ¶ˆæ¯ä½œä¸ºå¯¹è¯èµ·ç‚¹
"""
def clear_chat_history():
    st.session_state["chat_history"] = [
            {"role": "assistant", "content": CHAT_PAGE_INTRODUCTION}
        ]


def clear_chat_history():
    st.session_state["chat_history"] = [
            {"role": "assistant", "content": CHAT_PAGE_INTRODUCTION}
        ]
    

def main():
    load_dotenv()
    #st.set_page_config(page_title="Ask your PDF")
    st.header("Ask your PDF ğŸ’¬")

    with st._bottom:
        cols = st.columns([1.2, 10, 1])  # ä½¿ç”¨ä¸‰åˆ—å¸ƒå±€ï¼›é…ç½®æŒ‰é’®(1.2)ã€è¾“å…¥æ¡†(10)ã€æ¸…ç©ºæŒ‰é’®(1)
        with cols[0].popover(":gear:", use_container_width=True, help="é…ç½®æ¨¡å‹"):
            platform = st.selectbox("è¯·é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹åŠ è½½æ–¹å¼", PLATFORMS)
            llm_models = get_llm_models(platform)
            model = st.selectbox("è¯·é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹", llm_models)
            temperature = st.slider("è¯·é€‰æ‹©æ¨¡å‹ Temperature", 0.1, 1., 0.1)
            history_len = st.slider("è¯·é€‰æ‹©å†å²æ¶ˆæ¯é•¿åº¦", 1, 10, 5)
        user_question = cols[1].chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜")  # è¾“å…¥æ¡†
        cols[2].button(":wastebasket:", help="æ¸…ç©ºå¯¹è¯", on_click=clear_chat_history)  # æ¸…ç©ºæŒ‰é’®

    # upload file
    pdf = st.file_uploader("Upload your PDF", type="pdf")
    
    # extract the text
    if pdf is not None:
      pdf_reader = PdfReader(pdf)
      text = ""
      for page in pdf_reader.pages:
        text += page.extract_text()
        
      # split into chunks
      text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
      )
      chunks = text_splitter.split_text(text)
      
      # create embeddings
      embeddings = OpenAIEmbeddings()
      knowledge_base = FAISS.from_texts(chunks, embeddings)
      st.write("PDF Upload Sucessful")
      
      # show user input
      #user_question = st.text_input("Ask a question about your PDF:")
      if user_question:
        with st.chat_message("user"):
           st.write(user_question)

        docs = knowledge_base.similarity_search(user_question)
        llm = get_chatllm(platform, model, temperature=temperature)
        chain = load_qa_chain(llm, chain_type="stuff")
        with get_openai_callback() as cb:
          response = chain.run(input_documents=docs, question=user_question)
          print(cb)
        
        with st.chat_message("assistant"):
           answer = st.write(response)
        st.session_state["chat_history"] += [{'role': 'assistent', 'content': answer}]
    

# if __name__ == '__main__':
#     main()